<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="Dscribe the InternVL">
  <meta name="keywords" content="multimodal chatbot">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>InternVideo</title>

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.1/css/bulma.min.css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="https://github-production-user-asset-6210df.s3.amazonaws.com/47669167/330728723-7037290e-f474-4d11-b90f-1d8316087bf8.jpg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240529%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240529T072300Z&X-Amz-Expires=300&X-Amz-Signature=d12b9e5c3c49a082747f5da55529a4f1247cd17b4329fafc1cb6d1c0678efa77&X-Amz-SignedHeaders=host&actor_id=23737120&key_id=0&repo_id=721995615">
  <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">


  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/js/all.min.js"></script>
  <script type="module" src="https://gradio.s3-us-west-2.amazonaws.com/3.35.2/gradio.js"></script>
</head>

<style>
  .expandable-card .card-text-container {
    max-height: 200px;
    overflow-y: hidden;
    position: relative;
  }

  .expandable-card.expanded .card-text-container {
    max-height: none;
  }

  .expand-btn {
    position: relative;
    display: none;
    background-color: rgba(255, 255, 255, 0.8);
    /* margin-top: -20px; */
    /* justify-content: center; */
    color: #510c75;
    border-color: transparent;
  }

  .expand-btn:hover {
    background-color: rgba(200, 200, 200, 0.8);
    text-decoration: none;
    border-color: transparent;
    color: #510c75;
  }

  .expand-btn:focus {
    outline: none;
    text-decoration: none;
  }

  .expandable-card:not(.expanded) .card-text-container:after {
    content: "";
    position: absolute;
    bottom: 0;
    left: 0;
    width: 100%;
    height: 90px;
    background: linear-gradient(rgba(255, 255, 255, 0.2), rgba(255, 255, 255, 1));
  }

  .expandable-card:not(.expanded) .expand-btn {
    margin-top: -40px;
  }

  .card-body {
    padding-bottom: 5px;
  }

  .vertical-flex-layout {
    justify-content: center;
    align-items: center;
    height: 100%;
    display: flex;
    flex-direction: column;
    gap: 5px;
  }

  .figure-img {
    max-width: 100%;
    height: auto;
  }

  .adjustable-font-size {
    font-size: calc(0.5rem + 2vw);
  }

  .chat-history {
    flex-grow: 1;
    overflow-y: auto;
    /* overflow-x: hidden; */
    padding: 5px;
    border-bottom: 1px solid #ccc;
    margin-bottom: 10px;
  }

  .internvideo-title {
    color: #00008B; 
  }

  #gradio pre {
    background-color: transparent;
  }

  /* h2 {
    text-align: left;
    font-weight: bold;
  } */
</style>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">
              <span class="internvideo-title">InternVideo</span>: Video Foundation Models for Multimodal Understanding
            </h1>
            <h5 class="subtitle is-5 publication-awards"></h5>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <span class="internvideo-title">InternVideo</span> Team <br>
               OpenGVLab, Shanghai AI Laboratory
              </span>
            <div class="is-size-5 publication-authors">
            </div>
            <br>
            <!-- <hr> -->
            <br>

            
            <section>
              <h2 class="title"> InternVideo </h2>
              <ul class="post-list">
                <li>
                  <!-- <br>
                  <span class="post-meta">InternVideo </span>
                  
                  <br> -->
      
                  2022/12/06<br> <a class="post-link" href="blog/2022-12-06-InternVideo-1.0">
                      InternVideo: General Video Foundation Models via Generative and Discriminative Learning
                    </a>
                  <br>InternVideo efficiently explores masked video modeling and video-language contrastive learning as the pretraining objectives, and selectively coordinates video representations of these two complementary frameworks in a learnable manner to boost various video applications, obtaining <b>91.1%</b> and <b>77.2%</b> top-1 accuracy on the challenging Kinetics-400 and Sth-V2 benchmarks, respectively.
              
                  <br>
                  <!-- <span class="post-meta"></span> -->
         
                  2023/07/13<br> <a class="post-link" href="blog/2023-07-13-InternVid">
                      InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding and Generation
                    </a> <br>

                    A large-scale video-centric multimodal dataset that enables learning powerful and transferable video-text representations for multimodal understanding and generation. The InternVid dataset contains over <b>7 million videos lasting nearly 760K</b> hours, yielding <b>234M video clips</b> accompanied by detailed descriptions of total 4.1B words.
                    <br>
                    2024/03/22<br> <a class="post-link" href="blog/2024-03-22-InternVideo-2.0">
                     InternVideo2: Scaling Foundation Models for Multimodal Video Understanding
                    </a>
                    <br>
                    A new family of video foundation models (ViFM) that achieve the state-of-the-art results in video recognition, video-text tasks, and video-centric dialogue (<b>over 60 video and audio tasks</b>). Our core design is a progressive training approach that unifies the masked video modeling, crossmodal contrastive learning, and next token prediction, <b>scaling up the video encoder size to 6B parameters</b>.
                    <br>

                </li>
              </ul>
            </section>

            <section>
              <h2 class="title"> VideoChat </h2>
              <ul class="post-list">
                <li>
                  <!-- <br>
                  <span class="post-meta">InternVideo </span>
                  
                  <br> -->
      
                  2023/03/10<br> <a class="post-link" href="blog/xx">
                    VideoChat: Chat-centric Video Understanding
                    </a>
                  <br><b>First attempt</b> of developing an end-to-end chat-centric video understanding system, coined as <b>VideoChat</b>. It integrates video foundation models and large language models via a learnable neural interface, excelling in spatiotemporal reasoning, event localization, and causal relationship inference.
              
                  <br>
                  <!-- <span class="post-meta"></span> -->
         
                  2023/11/28<br> <a class="post-link" href="blog/xx">
                    MVBench: A Comprehensive Multi-modal Video Understanding Benchmark
                    </a> <br>
                    A comprehensive Multi-modal Video understanding Benchmark namely MVBench which covers 20 challenging video tasks that <b>cannot be effectively solved with a single frame</b>. Moreover we further develop a robust video MLLM baseline i.e. <b>VideoChat2</b> by progressive multi-modal training with diverse instruction-tuning data. 
                    <br>
                    2024/10/25<br> <a class="post-link" href="blog/xx">
                    TimeSuite: Improving MLLMs for Long Video Understanding via Grounded Tuning
                    </a>
                    <br>
                    A collection of new designs to adapt the existing short-form video MLLMs for long video understanding, including a simple yet efficient framework to process long video sequence, a high-quality video dataset for grounded tuning of MLLMs, and a carefully-designed instruction tuning task to explicitly incorporate the grounding supervision in the traditional QA format. Our VideoChat-T exhibits <b>robust zero-shot temporal grounding capabilities</b>, significantly outperforming the existing state-of-the-art MLLMs.
                    <br>

                </li>
              </ul>
            </section>
            <section class="section" id="BibTeX" style="background-color:#ffffffff">
              <div class="container is-max-desktop content">
                <h2 class="title">Citation</h2>
                <pre class="left-aligned smaller-font"><code>
@article{wang2024internvideo2,
title={Internvideo2: Scaling video foundation models for multimodal video understanding},
author={Wang, Yi and Li, Kunchang and Li, Xinhao and Yu, Jiashuo and He, Yinan and Wang, Chenting and Chen, Guo and Pei, Baoqi and Zheng, Rongkun and Xu, Jilan and Wang, Zun and others},
journal={arXiv preprint arXiv:2403.15377},
year={2024}
}
            
@article{wang2022internvideo,
title={InternVideo: General Video Foundation Models via Generative and Discriminative Learning},
author={Wang, Yi and Li, Kunchang and Li, Yizhuo and He, Yinan and Huang, Bingkun and Zhao, Zhiyu and Zhang, Hongjie and Xu, Jilan and Liu, Yi and Wang, Zun and Xing, Sen and Chen, Guo and Pan, Junting and Yu, Jiashuo and Wang, Yali and Wang, Limin and Qiao, Yu},
journal={arXiv preprint arXiv:2212.03191},
year={2022}
}
                </code></pre>
              </div>
            </section>
            <style>
              .left-aligned {
                text-align: left; /* Aligns text to the left */
              }
              .smaller-font {
                font-size: 0.8em; /* Reduces font size by 20% */
              }
            </style>

  <section class="section" id="Acknowledgement">
    <div class="container is-max-desktop content">
      <h2 class="title">Acknowledgement</h2>
      <p>
        This website is adapted from <a
        href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>, licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
        Commons Attribution-ShareAlike 4.0 International License</a>.  We thank the LLaMA team for giving us access to their models, and open-source projects, including Alpaca and Vicuna.
      </p>
    </div>
  </section>

</body>

</html>
